{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Control.Arrow (first)\n",
    "import Data.Text (pack)\n",
    "import Control.Monad\n",
    "\n",
    ":l Plotting.hs\n",
    ":l ../src/Control/Monad/Bayes/Class.hs\n",
    ":l ../src/Control/Monad/Bayes/Enumerator.hs\n",
    ":l ../src/Control/Monad/Bayes/Sampler.hs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Monad-Bayes\n",
    "\n",
    "This serves as an interactive alternative to [the user guide](https://monad-bayes.netlify.app/probprog.html). This isn't intended as a tutorial to Haskell, but if you're familiar with probabilistic programming, the general flow of the code should look familiar.\n",
    "\n",
    "To get a sense of how probabilistic programming with monad-bayes works, consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model :: MonadInfer m => Double -> m Double\n",
    "model observation = do\n",
    "    mean <- uniformD [-1, 0, 1]\n",
    "    factor (normalPdf mean 1 observation)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of monad-bayes, and probabilistic programming languages in general is to define distributions as programs. `model` corresponds to the distribution that you would express mathematically as:\n",
    "\n",
    "$$ P(m | o) = \\frac{P(m)P(o|m)}{P(o)} = \\frac{1/3 * \\mathbb{N}(o; m, 1)}{\\sum_{m' \\in \\{-1,0,1\\}} 1/3 * \\mathbb{N}(o; m', 1) } $$\n",
    "\n",
    "As a program, you can think of `model` as doing the following:\n",
    "\n",
    "- first draw from the prior over possible values of `mean` (that's the line `mean <- uniformD [-1, 0, 1]`)\n",
    "- then score a draw higher according to the likelihood placed on `observation` (the argument to `model`) by a normal with $\\mu$=`mean`\n",
    "\n",
    "- then return the `mean`\n",
    "\n",
    "To orient you on the relationship between the mathematical view of a distribution and the programming one, here are some notes:\n",
    "\n",
    "- a distribution over values of type `a` has type `MonadInfer m => m a`\n",
    "- a joint distribution over values of types `a` and `b` is a distribution over a tuple: `MonadInfer m => m (a, b)`\n",
    "- a conditional distribution over values of type `a` conditioned on values of type `b` is a function into a distribution: \n",
    "    `MonadInfer m => b -> m a`\n",
    "\n",
    "\n",
    "\n",
    "For example, if the value observed is $0.3$, then we can calculate the distribution over the mean:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-1.0,0.198111610867497),(0.0,0.44090549839518783),(1.0,0.36098289073731515)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inferredDistribution = enumerate $ model 0.3\n",
    "inferredDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v4+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.json",
       "data": {
        "values": [
         {
          "X": "-1.0",
          "Y": 0.198111610867497
         },
         {
          "X": "0.0",
          "Y": 0.44090549839518783
         },
         {
          "X": "1.0",
          "Y": 0.36098289073731515
         }
        ]
       },
       "encoding": {
        "x": {
         "field": "X",
         "type": "nominal"
        },
        "y": {
         "field": "Y",
         "type": "quantitative"
        }
       },
       "height": 200,
       "mark": "bar",
       "width": 200
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotVega (first (pack . show) <$> inferredDistribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce this plot, we performed *inference*, to obtain the exact form of the distribution represented by `model`. Because the only random variable in `model` had a support that was small and discrete (the set $\\{-1, 0, 1\\}$), performing this inference exactly was straightforward.\n",
    "\n",
    "`enumerate` is the exact inference method offered by monad-bayes.\n",
    "\n",
    "You are encouraged to change `model` in a number of ways and observe how the results change:\n",
    "- try changing the prior (currently `uniformD [-1, 0, 1]`)\n",
    "- try changing the score (currently `factor (normalPdf mean 1 observation)`)\n",
    "- try changing the types of the observation and latent variable (i.e. `mean`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with Haskell, then it should be clear that the class of distributions you can express in this way is very broad, since we have monadic control flow. For example, you could build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthDist :: MonadInfer m => Double -> m Int\n",
    "lengthDist observation = do\n",
    "    means <- replicateM 3 (model observation)\n",
    "    return (length $ filter (>=1) means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an observation, this is the probability on how many (out of 3) independent draws from the posterior of the model conditioned on the observation will be greater or equal to 1. Consider the hassle of defining this with an equation, and you'll see why probabilistic programming is appealing as a way of accelerating modelling and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,0.1927812120128709),(1,0.42280307652209714),(2,0.30909381616263754),(3,7.532189530239394e-2)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enumerate $ lengthDist 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v4+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.json",
       "data": {
        "values": [
         {
          "X": "0",
          "Y": 0.07884237924752763
         },
         {
          "X": "1",
          "Y": 0.31507728008413327
         },
         {
          "X": "2",
          "Y": 0.4197137519723294
         },
         {
          "X": "3",
          "Y": 0.18636658869600836
         }
        ]
       },
       "encoding": {
        "x": {
         "field": "X",
         "type": "nominal"
        },
        "y": {
         "field": "Y",
         "type": "quantitative"
        }
       },
       "height": 200,
       "mark": "bar",
       "width": 200
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plotVega $ fmap (first (pack . show)) $ enumerate $ lengthDist 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v4+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.json",
       "data": {
        "values": [
         {
          "X": "0",
          "Y": 0.3821154666057625
         },
         {
          "X": "1",
          "Y": 0.43338893548101026
         },
         {
          "X": "2",
          "Y": 0.1638474805061543
         },
         {
          "X": "3",
          "Y": 0.02064811740707282
         }
        ]
       },
       "encoding": {
        "x": {
         "field": "X",
         "type": "nominal"
        },
        "y": {
         "field": "Y",
         "type": "quantitative"
        }
       },
       "height": 200,
       "mark": "bar",
       "width": 200
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotVega $ fmap (first (pack . show)) $ enumerate $ lengthDist 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact sampling is pretty limited. For models with continuous random variables, or large discrete ones, it is a no-go. \n",
    "\n",
    "The broader goal is to be able to define your distribution of interest, like `model`, and then apply different inference technique, usually approximate, to it. This is what monad-bayes (and other probabilistic programming languages) enable. See the following tutorials for details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "mimetype": "text/x-haskell",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "8.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
